[NeMo W 2023-05-22 06:54:00 experimental:27] Module <class 'nemo.collections.tts.models.fastpitch_ssl.FastPitchModel_SSL'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-22 06:54:00 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-22 06:54:00 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-22 06:54:01 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-22 06:54:01 experimental:27] Module <class 'nemo.collections.tts.models.ssl_tts.SSLDisentangler'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-22 06:54:01 experimental:27] Module <class 'nemo.collections.tts.models.vits.VitsModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-22 06:54:01 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo I 2023-05-22 06:54:01 exp_manager:374] Experiments will be logged at ljspeech_to_9017_no_mixing_5_mins/FastPitch/2023-05-22_06-54-01
[NeMo I 2023-05-22 06:54:01 exp_manager:797] TensorboardLogger has been set up
[NeMo W 2023-05-22 06:54:01 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 25 epochs to ensure that checkpointing will not error out.
[NeMo W 2023-05-22 06:54:39 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.
[NeMo I 2023-05-22 06:54:39 dataset:227] Loading dataset from ./9017_manifest_train_dur_5_mins_local.json.
[NeMo I 2023-05-22 06:54:39 dataset:265] Loaded dataset with 76 files.
[NeMo I 2023-05-22 06:54:39 dataset:267] Dataset contains 0.08 hours.
[NeMo I 2023-05-22 06:54:39 dataset:375] Pruned 0 files. Final dataset contains 76 files
[NeMo I 2023-05-22 06:54:39 dataset:377] Pruned 0.00 hours. Final dataset contains 0.08 hours.
[NeMo W 2023-05-22 06:54:39 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(_create_warning_msg(
    
[NeMo I 2023-05-22 06:54:39 dataset:227] Loading dataset from ./9017_manifest_dev_ns_all_local.json.
[NeMo I 2023-05-22 06:54:39 dataset:265] Loaded dataset with 2 files.
[NeMo I 2023-05-22 06:54:39 dataset:267] Dataset contains 0.00 hours.
[NeMo I 2023-05-22 06:54:39 dataset:375] Pruned 0 files. Final dataset contains 2 files
[NeMo I 2023-05-22 06:54:39 dataset:377] Pruned 0.00 hours. Final dataset contains 0.00 hours.
[NeMo I 2023-05-22 06:54:39 features:287] PADDING: 1
[NeMo W 2023-05-22 06:55:17 fastpitch:209] This checkpoint support will be dropped after r1.18.0.
[NeMo W 2023-05-22 06:55:17 experimental:27] Module <class 'nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-22 06:55:17 en_us_arpabet:66] apply_to_oov_word=None, This means that some of words will remain unchanged if they are not handled by any of the rules in self.parse_one_word(). This may be intended if phonemes and chars are both valid inputs, otherwise, you may see unexpected deletions in your input.
[NeMo W 2023-05-22 06:55:17 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_train_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: 0.1
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: 24
      num_workers: 0
    
[NeMo W 2023-05-22 06:55:17 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    dataset:
      _target_: nemo.collections.tts.torch.data.TTSDataset
      manifest_filepath: /ws/LJSpeech/nvidia_ljspeech_val_clean_ngc.json
      sample_rate: 22050
      sup_data_path: /raid/LJSpeech/supplementary
      sup_data_types:
      - align_prior_matrix
      - pitch
      n_fft: 1024
      win_length: 1024
      hop_length: 256
      window: hann
      n_mels: 80
      lowfreq: 0
      highfreq: 8000
      max_duration: null
      min_duration: null
      ignore_file: null
      trim: false
      pitch_fmin: 65.40639132514966
      pitch_fmax: 2093.004522404789
      pitch_norm: true
      pitch_mean: 212.35873413085938
      pitch_std: 68.52806091308594
      use_beta_binomial_interpolator: true
    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: 24
      num_workers: 0
    
[NeMo I 2023-05-22 06:55:17 features:287] PADDING: 1
[NeMo I 2023-05-22 06:55:18 save_restore_connector:249] Model FastPitchModel was successfully restored from /mnt/tts_en_fastpitch_align.nemo.
[NeMo I 2023-05-22 06:55:18 modelPT:1221] Model checkpoint restored from nemo file with path : `./tts_en_fastpitch_align.nemo`
[NeMo I 2023-05-22 06:55:19 modelPT:722] Optimizer config = Adam (
    Parameter Group 0
        amsgrad: False
        betas: [0.9, 0.999]
        capturable: False
        differentiable: False
        eps: 1e-08
        foreach: None
        fused: None
        lr: 0.0002
        maximize: False
        weight_decay: 1e-06
    )
[NeMo I 2023-05-22 06:55:19 lr_scheduler:772] Scheduler not initialized as no `sched` config supplied to setup_optimizer()
[NeMo W 2023-05-22 06:55:24 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
      rank_zero_warn(
    
[NeMo I 2023-05-22 06:55:24 preemption:56] Preemption requires torch distributed to be initialized, disabling preemption
[NeMo W 2023-05-22 06:55:24 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(_create_warning_msg(
    
